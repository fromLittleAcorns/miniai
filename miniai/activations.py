# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/06_Activations.ipynb.

# %% ../nbs/06_Activations.ipynb 2
from __future__ import annotations
import random,math,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt
import fastcore.all as fc
from fastcore.test import test_close
from pathlib import Path
from operator import attrgetter,itemgetter
from functools import partial
from typing import Callable
import logging

from torch import tensor,nn,optim
import torch.nn.functional as F
import torchvision.transforms.functional as TF
from datasets import load_dataset

from .datasets import *
from .utils import set_seed, to_cpu
from .plotting import get_grid, show_image
from .callbacks import *

# %% auto 0
__all__ = ['Hook', 'append_stats', 'Hooks', 'HooksCallback', 'get_hist', 'get_min', 'ActivationStatsCB']

# %% ../nbs/06_Activations.ipynb 29
class Hook():
    """ Class to initilize and when necessary remove hooks in a Pytorch module.  The supplied
    function will be called after every forward call to the layer, when it will be called 
    with the layer input and outputs
    
    """
    def __init__(self, m: nn.Module, func: Callable): 
        """ Register the forward hook with the supplied module using the supplied function
        Note that the supplied function has to have signature of module, input, output
        
        args:
            m: The module layer to which the hook is to be assigned
            func: The function to be called immediately after any forward calls on the layer
        """
        self.hook = m.register_forward_hook(partial(func, self))
        
    def remove(self):
        self.hook.remove()
        
    def __del__(self):
        """ Ensure that the hook is removed when the class instance is deleted
        """
        self.remove()

# %% ../nbs/06_Activations.ipynb 30
def append_stats(hook: Hook, mod: nn.Module, inp: torch.Tensor, outp: torch.Tensor):
    """ Record the activations of model layers using a hook.  For the supplied hook a new parameter
    'stats' is added, which will record the means and std deviation for each call of the hook.
    Since this is called every time the layer is used the position in each list reflects the step
    
    Args:
        hook: the hook for which the activations are being recorded
        mod: the module of the model to which the hook is assigned (unused)
        inp: the input to the layer (unused)
        outp:the output activations
    
    """
    # Initialize hook by adding stats attribute.
    if not hasattr(hook, 'stats'):
        hook.stats = ([], [])
    acts = to_cpu(outp)
    hook.stats[0].append(acts.mean())
    hook.stats[1].append(acts.std())

# %% ../nbs/06_Activations.ipynb 44
class Hooks(list):
    """ Class to act as a container and context manager for a set of hooks and to ensure that they are 
    all removed at the end.  
    
    For the context manager the very least that is needed is an __enter__ and __exit__. 
    Inherits from the list class so that it is possible to iterate through members of the class as though
    it was a list.  This requires use of the list super class initilization
    
    Note that this works by iterating through layers as in an nn.Sequential model.  If the model has a
    different structure this class is likely to need modification
    
    The __del__ method is called to ensure that when the instance of the class is deleted then the associated
    hooks are also deleted
    
    """
    def __init__(self, mdl, func):
        # Initialize a hook for each layer and assign to a list
        super().__init__([Hook(l, func) for l in mdl])
    def __enter__(self, *args): return self
    def __exit__(self, *args): self.remove()
    def __del__(self):
        # triggered to delete the class.  This calls the remove method to remove all of the hooks
        self.remove()
    def __delitem__(self, i):
        # delete a specific hook from the model
        self[i].remove()
        # remove the hook from the list
        super().__delitem__(i)
    def remove(self): 
        for h in self: h.remove()

# %% ../nbs/06_Activations.ipynb 49
class HooksCallback(Callback):
    def __init__(self, hookfunc, module_filter=fc.noop, on_train: bool=True, on_valid:bool=False, 
                mods=None):
        """ mods should be a set of layers to which hooks should be applied.  If mods
        is supplied then module_filter is not used.
        
        args:
            hookfunc: Function that will be called on the layers selected
            module_filter: A function to use to select the layers to use (defaults to fc.noop, which
              will select all.  Typically use a function such as fc.filter_ex to define specific
              types of layers.  When using this approach with large models many layers will end 
              up with hooks attached and so it can be better to use the mods option below to select
              a specific list of layers
            on_train: If true then the hool will be applied on training
            on_valid: if true that applies the hook during validation
            mods: If supplied then should be a list of layers to which the hooks will be applied.
              When supplied then the module_filter is not used. The list can be indexed layers of 
              the model or a list created using model.named_children where that works.
              Defaults to None
            
        """
        fc.store_attr()
        super().__init__()
    
    def before_fit(self, learn): 
        if self.mods:
            mods = self.mods
        else:
            # get list of filtered layers
            mods = fc.filter_ex(learn.model.modules(), self.module_filter)
        # create hooks
        self.hooks = Hooks(mods, partial(self._hookfunc, learn))
    
    def _hookfunc(self, learn, *args, **kwargs):
        if (self.on_train and learn.training) or \
            (self.on_valid and not learn.training): self.hookfunc(*args, **kwargs)
    
    def after_fit(self, learn): self.hooks.remove()
    
    def __iter__(self): return iter(self.hooks)
    def __len__(self): return len(self.hooks)

# %% ../nbs/06_Activations.ipynb 70
def append_stats(hook: Hook, mod: nn.Module, inp: torch.Tensor, outp: torch.Tensor):
    """ Record the activations of model layers using a hook.  For the supplied hook a new parameter
    'stats' is added, which will record the means and std deviation for each call of the hook.
    Since this is called every time the layer is used the position in each list reflects the step
    The hook property will contain three lists, one for the activation means at each call, one for
    the standard deviation and one containing a histogram of the activations
    
    Args:
        hook: the hook for which the activations are being recorded
        mod: the module of the model to which the hook is assigned (unused)
        inp: the input to the layer (unused)
        outp:the output activations
    
    """
    if not hasattr(hook,'stats'): hook.stats = ([],[],[])
    acts = to_cpu(outp)
    hook.stats[0].append(acts.mean())
    hook.stats[1].append(acts.std())
    hook.stats[2].append(acts.abs().histc(bins=40, min=0, max=10))

# %% ../nbs/06_Activations.ipynb 72
# Thanks to @ste for initial version of histgram plotting code
def get_hist(h: Hook):
    """ Take the data gathered by a the HooksCallback and prepare in a form suitable
    for plotting.  Note that the log of the activation values is plotted
    args:
        h: the hook to be processed
        
    returns:
        A new array of stacked histograms that can be plotted as a density image
    """
    return torch.stack(h.stats[2]).t().float().log1p()

# %% ../nbs/06_Activations.ipynb 79
def get_min(h):
    """ Calculate the proportion of activations in the smallest bin
    """
    h_arr = torch.stack(h.stats[2]).t().float()
    return h_arr[0] / h_arr.sum(0)

# %% ../nbs/06_Activations.ipynb 84
class ActivationStatsCB(HooksCallback):
    """ Adds visulisation capability to the HookCallback.
    Three types of chart can be produced by the class methods.

    color_dim: 
    """
    def __init__(self, module_filter=fc.noop):
        super().__init__(append_stats, module_filter)
        
    def color_dim(self, figsize=(11,5)):
        fig,axes = get_grid(len(self), figsize=figsize)
        for ax,h in zip(axes.flat, self):
            show_image(get_hist(h), ax, origin='lower')
    
    def dead_chart(self, figsize=(11,5)):
        fig,axes = get_grid(len(self), figsize=(11,5))
        for ax,h in zip(axes.flatten(), self):
            ax.plot(get_min(h))
            ax.set_ylim(0,1)

    def plot_stats(self, figsize=(10,4)):
        fig,axs = plt.subplots(1,2, figsize=figsize)
        for h in self:
            for i in 0,1: 
                axs[i].plot(h.stats[i])
        axs[0].set_title('Means')
        axs[1].set_title('Stdevs')
        plt.legend(fc.L.range(self))
