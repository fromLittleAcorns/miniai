# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/06_Activations.ipynb.

# %% ../nbs/06_Activations.ipynb 2
from __future__ import annotations
import random,math,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt
import fastcore.all as fc
from pathlib import Path
from operator import attrgetter,itemgetter
from functools import partial
from typing import Callable

from torch import tensor,nn,optim
import torch.nn.functional as F
import torchvision.transforms.functional as TF
from datasets import load_dataset

from .datasets import *
from .learner import *
from .utils import set_seed, to_cpu

# %% auto 0
__all__ = ['Hook', 'append_stats', 'Hooks', 'HooksCallback', 'get_hist', 'get_min', 'ActivationStatsCB']

# %% ../nbs/06_Activations.ipynb 28
class Hook():
    """ Class to initilize and when necessary remove hooks in a Pytorch module.  The supplied
    function will be called after every forward call to the layer, when it will be called 
    with the layer input and outputs
    
    """
    def __init__(self, m: nn.Module, func: Callable): 
        """ Register the forward hook with the supplied module using the supplied function
        Note that the supplied function has to have signature of module, input, output
        
        args:
            m: The module layer to which the hook is to be assigned
            func: The function to be called immediately after any forward calls on the layer
        """
        self.hook = m.register_forward_hook(partial(func, self))
        
    def remove(self):
        self.hook.remove()
        
    def __del__(self):
        """ Ensure that the hook is removed when the class instance is deleted
        """
        self.remove()

# %% ../nbs/06_Activations.ipynb 29
def append_stats(hook: Hook, mod: nn.Module, inp: torch.Tensor, outp: torch.Tensor):
    """ Record the activations of model layers using a hook.  For the supplied hook a new parameter
    'stats' is added, which will record the means and std deviation for each call of the hook.
    Since this is called every time the layer is used the position in each list reflects the step
    
    Args:
        hook: the hook for which the activations are being recorded
        mod: the module of the model to which the hook is assigned (unused)
        inp: the input to the layer (unused)
        outp:the output activations
    
    """
    # Initialize hook by adding stats attribute.
    if not hasattr(hook, 'stats'):
        hook.stats = ([], [])
    acts = to_cpu(outp)
    hook.stats[0].append(acts.mean())
    hook.stats[1].append(acts.std())

# %% ../nbs/06_Activations.ipynb 43
class Hooks(list):
    """ Class to act as a container and context manager for a set of hooks and to ensure that they are 
    all removed at the end.  
    
    For the context manager the very least that is needed is an __enter__ and __exit__. 
    Inherits from the list class so that it is possible to iterate through members of the class as though
    it was a list.  This requires use of the list super class initilization
    
    Note that this works by iterating through layers as in an nn.Sequential model.  If the model has a
    different structure this class is likely to need modification
    
    The __del__ method is called to ensure that when the instance of the class is deleted then the associated
    hooks are also deleted
    
    """
    def __init__(self, mdl, func):
        # Initialize a hook for each layer and assign to a list
        super().__init__([Hook(l, func) for l in mdl])
    def __enter__(self, *args): return self
    def __exit__(self, *args): self.remove()
    def __del__(self):
        # triggered to delete the class.  This calls the remove method to remove all of the hooks
        self.remove()
    def __delitem__(self, i):
        # delete a specific hook from the model
        self[i].remove()
        # remove the hook from the list
        super().__delitem__(i)
    def remove(self): 
        for h in self: h.remove()

# %% ../nbs/06_Activations.ipynb 48
class HooksCallback(Callback):
    def __init__(self, hookfunc, module_filter=fc.noop, on_train: bool=True, on_valid:bool=False, 
                mods=None):
        """ mods should be a set of layers to which hooks should be applied.  If mods
        is supplied then module_filter is not used.
        
        args:
            hookfunc: Function that will be called on the layers selected
            module_filter: A function to use to select the layers to use (defaults to fc.noop, which
              will select all.  Typically use a function such as fc.filter_ex to define specific
              types of layers
            on_train: If true then the hool will be applied on training
            on_valid: if true that applies the hook during validation
            mods: If supplied then should be a list of layers to which the hooks will be applied.
              When supplied then the module_filter is not used.  Defaults to None
            
        """
        fc.store_attr()
        super().__init__()
    
    def before_fit(self, learn): 
        if self.mods:
            mods = self.mods
        else:
            # get list of filtered layers
            mods = fc.filter_ex(learn.model.modules(), self.module_filter)
        # create hooks
        self.hooks = Hooks(mods, partial(self._hookfunc, learn))
    
    def _hookfunc(self, learn, *args, **kwargs):
        if (self.on_train and learn.training) or \
            (self.on_valid and not learn.training): self.hookfunc(*args, **kwargs)
    
    def after_fit(self, learn): self.hooks.remove()
    
    def __iter__(self): return iter(self.hooks)
    def __len__(self): return len(self.hooks)

# %% ../nbs/06_Activations.ipynb 56
def append_stats(hook, mod, inp, outp):
    if not hasattr(hook,'stats'): hook.stats = ([],[],[])
    acts = to_cpu(outp)
    hook.stats[0].append(acts.mean())
    hook.stats[1].append(acts.std())
    hook.stats[2].append(acts.abs().histc(40,0,10))

# %% ../nbs/06_Activations.ipynb 58
# Thanks to @ste for initial version of histgram plotting code
def get_hist(h): return torch.stack(h.stats[2]).t().float().log1p()

# %% ../nbs/06_Activations.ipynb 65
def get_min(h):
    """ Calculate the proportion of activations in the smallest bin
    """
    h_arr = torch.stack(h.stats[2]).t().float()
    return h_arr[0] / h_arr.sum(0)

# %% ../nbs/06_Activations.ipynb 70
class ActivationStatsCB(HooksCallback):
    """ Note that this builds upon the earlier work.  I have deviated from the course by passing in the 
    hookfunc since I think this gived greater flexibility moving forwards, although so many things are 
    now linked to the structure of the appstats function that it might be better to simply integrate that
    into this class
    """
    def __init__(self, hookfunc, module_filter=fc.noop):
        super().__init__(hookfunc, module_filter)
        
    def color_dim(self, figsize=(11,5)):
        fig,axes = get_grid(len(self), figsize=figsize)
        for ax,h in zip(axes.flat, self):
            show_image(get_hist(h), ax, origin='lower')
    
    def dead_chart(self, figsize=(11,5)):
        fig,axes = get_grid(len(self), figsize=(11,5))
        for ax,h in zip(axes.flatten(), self):
            ax.plot(get_min(h))
            ax.set_ylim(0,1)

    def plot_stats(self, figsize=(10,4)):
        fig,axs = plt.subplots(1,2, figsize=figsize)
        for h in self:
            for i in 0,1: 
                axs[i].plot(h.stats[i])
        axs[0].set_title('Means')
        axs[1].set_title('Stdevs')
        plt.legend(fc.L.range(self))
