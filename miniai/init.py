# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/09_Initiations.ipynb.

# %% auto 0
__all__ = ['BatchTransformCB', 'plot_func', 'init_weights', 'lsuv_init']

# %% ../nbs/09_Initiations.ipynb 2
import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt
import sys,gc,traceback
import fastcore.all as fc
from collections.abc import Mapping
from pathlib import Path
from operator import attrgetter,itemgetter
from functools import partial
from copy import copy
from contextlib import contextmanager

import torchvision.transforms.functional as TF,torch.nn.functional as F
from torch import tensor,nn,optim
from torch.utils.data import DataLoader,default_collate
from torch.nn import init
from torcheval.metrics import MulticlassAccuracy
from datasets import load_dataset,load_dataset_builder

from .datasets import *
from .utils import set_seed, def_device
from .callbacks import *
from .learner import *
from .activations import *
from .layers import GeneralRelu
from .model_blocks import conv

# %% ../nbs/09_Initiations.ipynb 12
class BatchTransformCB(Callback):
    def __init__(self, tfm, on_train=True, on_val=True): fc.store_attr()
    
    def before_batch(self, learn):
        if (self.on_train and learn.training) or (self.on_val and not learn.training): 
            learn.batch = self.tfm(learn.batch)

# %% ../nbs/09_Initiations.ipynb 14
def plot_func(f, start=-5, end=5, steps=100):
    x = torch.linspace(start, end, steps)
    plt.plot(x, f(x))
    plt.grid(visible=True, which='both', ls='--')
    plt.axhline(y=0, color='k', linewidth=1.0)
    plt.axvline(x=0, color='k', linewidth=1.0)

# %% ../nbs/09_Initiations.ipynb 15
def init_weights(m, leaky=0.):
    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d)): init.kaiming_normal_(m.weight, a=leaky)

# %% ../nbs/09_Initiations.ipynb 16
def _lsuv_stats(hook, mod, inp, outp):
    """Calculate stats for a specific module given the input and output values.  Assigns the mean and std
    as properties of the hook
    """
    acts = to_cpu(outp)
    hook.mean = acts.mean()
    hook.std = acts.std()
    
def lsuv_init(m, m_in, xb):
    """ Setput hook for specific module (one of the activation layer outputs usually).  Run a batch of 
    data trhough the model and adjust the weights of the layer feeding the hooked layer to bring the mean
    and std deviation at the end of thta layer to the target values
    
    args:
        m: layer to apply hook to.  Usually the output of an activation
        m_in: layer prior to the activation
        xb: a batch of data
    """
    h = Hook(m, _lsuv_stats)
    with torch.no_grad():
        while model(xb) is not None and (abs(h.mean)>1e-3 or (abs(h.std-1)>1.e-3)):
            m_in.bias -= h.mean
            m_in.weight.data /= h.std
    h.remove()
