# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/11_layers.ipynb.

# %% auto 0
__all__ = ['GeneralRelu']

# %% ../nbs/11_layers.ipynb 2
import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms.functional as TF,torch.nn.functional as F
import fastcore.test as fct

# %% ../nbs/11_layers.ipynb 3
class GeneralRelu(nn.Module):
    """ Extension of leaky relu with the option to limit the max value as well as subtract a constant from the 
    output of the leaky relu (presumably to move the transition point away from zero
    """
    def __init__(self, leak=None, sub=None, maxv=None):
        super().__init__()
        self.leak, self.sub, self. maxv = leak, sub, maxv
        
    def forward(self, x):
        x = F.leaky_relu(x, self.leak) if self.leak is not None else F.relu(x)
        if self.sub is not None: x -= self.sub
        if self.maxv is not None: x = x.clamp_max_(self.maxv)
        return x
