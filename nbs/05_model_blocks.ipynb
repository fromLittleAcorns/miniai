{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7d452e5-c5e1-48d6-8314-9a9dd8605aac",
   "metadata": {},
   "source": [
    "# Model Blocks\n",
    "\n",
    "Contains the building blocks for models in including conv blocks, resblocks etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b91c5-f072-4a65-a938-09b0de416297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b21df-5aae-4cca-8629-040b5f061d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\n",
    "import pandas as pd,matplotlib.pyplot as plt\n",
    "from functools import partial, wraps\n",
    "from pathlib import Path\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "import fastcore.test as fct\n",
    "import fastcore.all as fc\n",
    "\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "from typing import Mapping\n",
    "\n",
    "from miniai.datasets import *\n",
    "from miniai.layers import GeneralRelu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf9a4d2-aa94-4bb0-9a5b-2c99a547381c",
   "metadata": {},
   "source": [
    "Add a basic convolutional block, which is a fundamental building block of more complex blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0390da-122d-4511-9e46-18279ff05ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Temp - to be removed once activations module in place\n",
    "class GeneralRelu(nn.Module):\n",
    "    def __init__(self, leak=None, sub=None, maxv=None):\n",
    "        super().__init__()\n",
    "        self.leak, self.sub, self. maxv = leak, sub, maxv\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(x, self.leak) if self.leak is not None else F.relu(x)\n",
    "        if self.sub is not None: x -= self.sub\n",
    "        if self.maxv is not None: x = x.clamp_max_(self.maxv)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54641cb1-1654-4246-a4bd-e4b8a1915b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Temp - to be removed once activations module in place\n",
    "act_gr = partial(GeneralRelu, leak=0.1, sub=0.4)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "643be912-6f5c-497c-ba68-a845c51a467b",
   "metadata": {},
   "source": [
    "#|export\n",
    "def conv(\n",
    "    ni: int, # Input length\n",
    "    nch: int, # number of channels output\n",
    "    ks: int=3, # Kernel size (should be an odd number)\n",
    "    stride: int=2, # Stride\n",
    "    act: bool=True # Whether to assign an activation layer to the output\n",
    "):\n",
    "    out = nn.Conv2d(ni, nch, kernel_size=ks, stride=stride, padding=ks//2)\n",
    "    if act: out = nn.Sequential(out, nn.ReLU())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49625b1-6687-4469-94e0-1cb83a2437ba",
   "metadata": {},
   "source": [
    "### Conv Block\n",
    "Conv from notebook 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab78f9-77f3-4a64-824b-0e9703cd343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def conv(ni, # Input filters\n",
    "         nf, # Output filters\n",
    "         ks=3, # Kernel size\n",
    "         stride=2, # Stride,\n",
    "         padding=None, # Padding\n",
    "         act=nn.ReLU, # Activation\n",
    "         norm=None, # Type of normalization layer to apply\n",
    "         bias=None # Whether to apply bias\n",
    "        )-> nn.Sequential:\n",
    "    \"\"\" Generate a conv block with a conv layer and optional normalisation and activation. If bias \n",
    "    is None then the bias is not applied  to the conv if batch norm is used, otherwise it is\n",
    "\n",
    "    Using ks=3 and padding=1 will result in the resolution reducing as per the stride, as will \n",
    "    ks=5 and padding=2\n",
    "\n",
    "    Returns the block as a sequential model\n",
    "    \"\"\"\n",
    "    if bias is None:\n",
    "        bias = not norm in (torch.nn.modules.batchnorm.BatchNorm1d, \n",
    "                            torch.nn.modules.batchnorm.BatchNorm2d, \n",
    "                            torch.nn.modules.batchnorm.BatchNorm3d)\n",
    "    if padding is None: padding=ks//2\n",
    "    layers = [nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=padding, bias=bias)]\n",
    "    if norm: layers.append(norm(nf))\n",
    "    if act: layers.append(act())\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3764d0-8d5d-43b9-bbe0-6ac7ab8af444",
   "metadata": {},
   "source": [
    "#### Conv Block Tests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a21034-b310-4f24-b064-dff68d12fb63",
   "metadata": {},
   "source": [
    "Check conv block with defaults has the correct layers and properties.  Note that further tests can be added, these are basic starters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24847d1-71d3-48b6-80c9-7216729cdaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check conv block with defaults\n",
    "ni=3\n",
    "nf=6\n",
    "stride=2\n",
    "padding=1\n",
    "act=nn.ReLU\n",
    "norm=None\n",
    "bias=None\n",
    "conv_block = conv(ni=ni, nf=nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829efa67-4fea-4234-a844-ef580b51cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_bias(layer):\n",
    "    try:\n",
    "        bias = layer.get_parameter('bias')\n",
    "    except:\n",
    "        bias = None\n",
    "    return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3493ff-e408-4661-b26c-dad74c3f9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias should exist since there is no norm by default\n",
    "assert check_if_bias(conv_block[0]) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d54ea-448a-4e7b-8939-e65adaccd0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm there are only two layers (conv and activation)\n",
    "assert len(conv_block) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b0bbf-8f41-4a79-bc6f-60d408a29a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that when batchnorm is applied then the bias is deactivated unless specified\n",
    "norm = nn.BatchNorm2d\n",
    "bias = None\n",
    "conv_block = conv(ni=ni, nf=nf, norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928eaee-e89b-4ebd-b4fc-773a2e0a1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert check_if_bias(conv_block[0]) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27d460-f479-424f-b8bd-245a7b1b8608",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(conv_block) ==3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838fc7b4-690c-4f1b-8054-930daa5e1f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that when batchnorm is applied then and bias is specified then it is assigned\n",
    "norm=nn.BatchNorm2d\n",
    "bias=True\n",
    "conv_block = conv(ni=ni, nf=nf, norm=norm, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe94e0b-2cc1-4005-aa6c-52f71d0d4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert check_if_bias(conv_block[0]) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837af15f-8e00-4a46-aed8-59c1547e694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that if activation is None then that layer is not added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9011b9-a3de-4493-946c-4677ce7bc3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_block = conv(ni=ni, nf=nf, norm=norm, bias=bias, act=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067be4ad-a7f0-4109-9247-cf54f93dfa04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_block[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5124c-3ff8-4426-90cb-5ea419c9a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(conv_block[-1], nn.BatchNorm2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abae0a85-845b-4318-882b-7607de5add75",
   "metadata": {},
   "source": [
    "### ResBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a451564-1b4d-4045-bcca-4320baeacd6a",
   "metadata": {},
   "source": [
    "Add a conv_block which is the two conv layers used in the ResBlock (from notebook 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f953978-e6b0-41b3-a41e-dc2df4934aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _conv_block(ni, # input channels\n",
    "                nf, # out channels\n",
    "                stride, # stride\n",
    "                ks=3, # kernel size\n",
    "                act=act_gr, # activation to use\n",
    "                norm=None # normalization to use\n",
    "               ):\n",
    "    \"\"\" Generates a sequential model consisting of two conv blocks.  Note that the architectual \n",
    "    choice being made here is that the first conv changes the number of channels and the second \n",
    "    keeps the number of channels the same but reduces the resolution by using stride=2\n",
    "\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        conv(ni, nf, stride=1, ks=ks, act=act, norm=norm),\n",
    "        conv(nf, nf, stride=stride, ks=ks, act=None, norm=norm)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f68083-0476-4b5e-8d30-c2d74376c336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\" Create a traditional Resnet block with a conv block, a pass through path, a pooling layer and\n",
    "    an activation.\n",
    "  \n",
    "    The pass through block uses a pooling layer to reduce the resolution if required and then a \n",
    "    basic conv to change the number of channels to that required to facilitate addition to the \n",
    "    conv block output\n",
    "\n",
    "    The forward method will feed data though the block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 ni, # input channels\n",
    "                 nf, # out channels\n",
    "                 stride=1, # stride\n",
    "                 ks=3, # kernel size\n",
    "                 padding=None, # padding\n",
    "                 act=act_gr, # activation to use\n",
    "                 norm=None # normalization to use\n",
    "                ):\n",
    "        super().__init__()\n",
    "        # Create the two convolution layers\n",
    "        self.convs = _conv_block(ni, nf, stride, ks=ks, act=act, norm=norm)\n",
    "        # Create the pass through layer.  Note that this can only be a complete pass of the input if ni=nf.\n",
    "        # Where this is not the case a single conv is used (with no activation and kernel size of 1)\n",
    "        self.idconv = fc.noop if ni==nf else conv(ni, nf, stride=1, ks=1, act=None)\n",
    "        self.pool = fc.noop if stride==1 else nn.AvgPool2d(kernel_size=2, ceil_mode=True)\n",
    "        self.act = act()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.act(self.convs(x) + self.idconv(self.pool(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fad633-2efa-4dcb-a586-71f3fd2f197b",
   "metadata": {},
   "source": [
    "### Pre_conv\n",
    "\n",
    "From notebook 28\n",
    "Similar to a conv block but with the norm and activation layers ahead of the conv layers.  Found to work better in some cases including stable diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02095f2-c9e1-41fa-956f-229fdc561495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def pre_conv(ni, # input channels\n",
    "             nf, # out channels\n",
    "             ks=3, # kernel size\n",
    "             stride=1, # stride\n",
    "             act=nn.SiLU, # activation to use\n",
    "             norm=None, # normalization to use\n",
    "             bias=True # whether to use bias for the conv layer\n",
    "            ):\n",
    "    \"\"\" Generate a conv block with a conv layer and optional normalisation and activation.\n",
    "    Bias and norm layers are optional. Using ks=3 and padding=1 will result in the resolution \n",
    "    reducing as per the stride, as will ks=5 and padding=2\n",
    "\n",
    "    Returns the block as a sequential model\n",
    "    \"\"\"\n",
    "    layers = nn.Sequential()\n",
    "    if norm: layers.append(norm(ni))\n",
    "    if act : layers.append(act())\n",
    "    layers.append(nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba760ea6-9f6f-47e9-8989-03097b2c2959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: tests for pre_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5131548-5769-4792-9498-a47196b35397",
   "metadata": {},
   "source": [
    "### Upsample\n",
    "From notebook 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9bca8a-1d53-42f7-a789-0a7eb6e31cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(nf): return nn.Sequential(nn.Upsample(scale_factor=2.), nn.Conv2d(nf, nf, 3, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b4df45-7549-456f-880b-718941d9c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add tests for upsample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab282359-e3da-4fbc-a7e2-120667b5957d",
   "metadata": {},
   "source": [
    "### Linear Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8e8e2a-3c5b-4c2f-8fd2-a00a66c8e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def lin(ni, #input channels\n",
    "        nf, #output channels\"\n",
    "        act=nn.SiLU, # activation to use or None\n",
    "        norm=None, # normalisation to use or None\n",
    "        bias=True, # Whether to use bias in the linear layer\n",
    "       ):\n",
    "    \"\"\" Create a sequential model of a linear layer with optional bias and optional normalization and activation layers\n",
    "    \"\"\"\n",
    "    layers = nn.Sequential()\n",
    "    if norm: layers.append(norm(ni))\n",
    "    if act : layers.append(act())\n",
    "    layers.append(nn.Linear(ni, nf, bias=bias))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f898e0-6820-49b6-83a4-d7c6bc022109",
   "metadata": {},
   "source": [
    "### Self Attention (1D) \n",
    "From notebook 28 - documentation to be added\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb057a-d4e2-4244-a5de-37c42b7af955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni, attn_chans, transpose=True):\n",
    "        super().__init__()\n",
    "        self.nheads = ni//attn_chans\n",
    "        self.scale = math.sqrt(ni/self.nheads)\n",
    "        self.norm = nn.LayerNorm(ni)\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "        self.t = transpose\n",
    "    \n",
    "    def forward(self, x):\n",
    "        n,c,s = x.shape\n",
    "        if self.t: x = x.transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        x = self.qkv(x)\n",
    "        x = rearrange(x, 'n s (h d) -> (n h) s d', h=self.nheads)\n",
    "        q,k,v = torch.chunk(x, 3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = rearrange(x, '(n h) s d -> n s (h d)', h=self.nheads)\n",
    "        x = self.proj(x)\n",
    "        if self.t: x = x.transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a06b7a-fb84-4ca4-9d31-02428196f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add tests for attention module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16edd944-01d5-4fa8-8ec2-faea2bf58f7b",
   "metadata": {},
   "source": [
    "### Self Attention (2D)\n",
    "From notebook 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1752b-9d23-4020-9dc2-28b89c154783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class SelfAttention2D(SelfAttention):\n",
    "    def forward(self, x):\n",
    "        n,c,h,w = x.shape\n",
    "        return super().forward(x.view(n, c, -1)).reshape(n,c,h,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8376d0-3b13-49a2-9936-ffc1464e6aad",
   "metadata": {},
   "source": [
    "### Resblock with PreConv \n",
    "From notebook 28.  EmbResBlock is used to combine the timestep embedding for stable diffusion with the Res block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf60aa14-594b-4592-9c3a-2515e4f7e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class EmbResBlock(nn.Module):\n",
    "    def __init__(self, n_emb, ni, nf=None, ks=3, act=nn.SiLU, norm=nn.BatchNorm2d, attn_chans=0):\n",
    "        super().__init__()\n",
    "        if nf is None: nf = ni\n",
    "        self.emb_proj = nn.Linear(n_emb, nf*2)\n",
    "        self.conv1 = pre_conv(ni, nf, ks, act=act, norm=norm)\n",
    "        self.conv2 = pre_conv(nf, nf, ks, act=act, norm=norm)\n",
    "        self.idconv = fc.noop if ni==nf else nn.Conv2d(ni, nf, 1)\n",
    "        self.attn = False\n",
    "        if attn_chans: self.attn = SelfAttention2D(nf, attn_chans)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        inp = x\n",
    "        x = self.conv1(x)\n",
    "        emb = self.emb_proj(F.silu(t))[:, :, None, None]\n",
    "        scale,shift = torch.chunk(emb, 2, dim=1)\n",
    "        x = x*(1+scale) + shift\n",
    "        x = self.conv2(x)\n",
    "        x = x + self.idconv(inp)\n",
    "        if self.attn: x = x + self.attn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106ceca-c796-4bcd-a2f7-45a31f0b44ea",
   "metadata": {},
   "source": [
    "### Saved\n",
    "From notebook 28\n",
    "\n",
    "Function to enable activations from a down block to be saved to use in the up block of a Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771552a-d6a1-4164-8a7c-8ffc6cd6a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def saved(m, # torch.nn.module, the module for which the output will be saved\n",
    "          blk # The block containing the module\n",
    "         ):\n",
    "    \"\"\"Creates a function that will save the values of the embedding layers to facilitate passing to the \n",
    "    decoding part.  Depends upon the calling block containing a 'saved\" attribute as a list. Each module in the block \n",
    "    for which this is called will have its exit activations saved in the list\n",
    "\n",
    "    The wraps library simply ensures that any documentation of the wrapped method is available to the parent\n",
    "    \"\"\"\n",
    "    m_ = m.forward\n",
    "\n",
    "    @wraps(m.forward)\n",
    "    def _f(*args, **kwargs):\n",
    "        res = m_(*args, **kwargs)\n",
    "        blk.saved.append(res)\n",
    "        return res\n",
    "\n",
    "    m.forward = _f\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b83963b-3153-4a8f-aba2-71c5acca5d49",
   "metadata": {},
   "source": [
    "#### Tests for \"saved\" function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3992a61-ac91-47c2-92c6-2f26ae8a8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two conv layers with random initialization\n",
    "nf=5\n",
    "mod1 = nn.Conv2d(nf, nf, 3, stride=2, padding=1)\n",
    "mod2 = nn.Conv2d(nf, nf, 3, stride=1, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c1f2da-cc8c-4c28-9f14-8f7d590291d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestBlk(nn.Module):\n",
    "    \"\"\" Class to test that the saved function saves the output of pytorch modules within a block.\n",
    "    The saved function should append module ouputs into a class attribute saved in order of application, \n",
    "    hence in this class the self.saved list should end up with two entries, the first the output of the self.mod1\n",
    "    and the second the output of self.mod2\n",
    "    \"\"\"\n",
    "    def __init__(self, nf, mod1, mod2):\n",
    "        super().__init__()\n",
    "        self.mod1 = saved(mod1, self)\n",
    "        self.mod2 = saved(mod2, self)\n",
    "        self.nf = nf\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.saved=[]\n",
    "        mod1_out = self.mod1(x)\n",
    "        mod2_out = self.mod2(mod1_out)\n",
    "        return mod1_out, mod2_out      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa21c2-67c8-4d12-a8a9-07cd3c3a90cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input array and modules to test the functionality\n",
    "inp_arr = torch.rand([4, 5, 10, 10])\n",
    "out_m1 = mod1(inp_arr)\n",
    "out_m2 = mod2(out_m1)\n",
    "\n",
    "test_save = TestBlk(nf, mod1, mod2)\n",
    "mod1_out, mod2_out = test_save(inp_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15ac2c-7105-4d18-ae97-8b47dac318f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(test_save.saved) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a963515-dc45-4fcf-9e9c-02923cff2e21",
   "metadata": {},
   "source": [
    "Check that the saved arrays match the values calculated outside of the class, which they should"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c56d8-118a-4c8f-909a-7ec3496e0c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fct.array_equal(out_m1.detach().numpy(), test_save.saved[0].detach().numpy()) & \\\n",
    "fct.array_equal(out_m2.detach().numpy(), test_save.saved[1].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1d676-c9b9-4cc1-8241-2ae136f39ec2",
   "metadata": {},
   "source": [
    "### DownBlock "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cf91e3-cff6-4770-8820-2d672b84cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\" A down block is a part of a stable diffusion Unet.  It contains an EmbResBlock which is followed \n",
    "    by an optional down block (if no down block then an identity is used).  Activations of teh EmbResBlock\n",
    "    and the down block are saved for use as cross connections for the corresponding up blocks\n",
    "    \"\"\"\n",
    "    def __init__(self, n_emb, ni, nf, add_down=True, num_layers=1, attn_chans=0):\n",
    "        super().__init__()\n",
    "        self.resnets = nn.ModuleList([saved(EmbResBlock(n_emb, ni if i==0 else nf, nf, attn_chans=attn_chans), self)\n",
    "                                      for i in range(num_layers)])\n",
    "        self.down = saved(nn.Conv2d(nf, nf, 3, stride=2, padding=1), self) if add_down else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.saved = []\n",
    "        for resnet in self.resnets: x = resnet(x, t)\n",
    "        x = self.down(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f425211-69bd-4e93-bbf5-2f70aad794da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, n_emb, ni, prev_nf, nf, add_up=True, num_layers=2, attn_chans=0):\n",
    "        super().__init__()\n",
    "        self.resnets = nn.ModuleList(\n",
    "            [EmbResBlock(n_emb, (prev_nf if i==0 else nf)+(ni if (i==num_layers-1) else nf), nf, attn_chans=attn_chans)\n",
    "            for i in range(num_layers)])\n",
    "        self.up = upsample(nf) if add_up else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t, ups):\n",
    "        for resnet in self.resnets: x = resnet(torch.cat([x, ups.pop()], dim=1), t)\n",
    "        return self.up(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64106b6c-ec97-40d8-b292-cf09eb4ae977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EmbUNetModel(nn.Module):\n",
    "    def __init__( self, in_channels=3, out_channels=3, nfs=(224,448,672,896), num_layers=1, attn_chans=8, attn_start=1):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(in_channels, nfs[0], kernel_size=3, padding=1)\n",
    "        self.n_temb = nf = nfs[0]\n",
    "        n_emb = nf*4\n",
    "        self.emb_mlp = nn.Sequential(lin(self.n_temb, n_emb, norm=nn.BatchNorm1d),\n",
    "                                     lin(n_emb, n_emb))\n",
    "        self.downs = nn.ModuleList()\n",
    "        n = len(nfs)\n",
    "        for i in range(n):\n",
    "            ni = nf\n",
    "            nf = nfs[i]\n",
    "            self.downs.append(DownBlock(n_emb, ni, nf, add_down=i!=n-1, num_layers=num_layers,\n",
    "                                        attn_chans=0 if i<attn_start else attn_chans))\n",
    "        self.mid_block = EmbResBlock(n_emb, nfs[-1])\n",
    "\n",
    "        rev_nfs = list(reversed(nfs))\n",
    "        nf = rev_nfs[0]\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i in range(n):\n",
    "            prev_nf = nf\n",
    "            nf = rev_nfs[i]\n",
    "            ni = rev_nfs[min(i+1, len(nfs)-1)]\n",
    "            self.ups.append(UpBlock(n_emb, ni, prev_nf, nf, add_up=i!=n-1, num_layers=num_layers+1,\n",
    "                                    attn_chans=0 if i>=n-attn_start else attn_chans))\n",
    "        self.conv_out = pre_conv(nfs[0], out_channels, act=nn.SiLU, norm=nn.BatchNorm2d, bias=False)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x,t = inp\n",
    "        temb = timestep_embedding(t, self.n_temb)\n",
    "        emb = self.emb_mlp(temb)\n",
    "        x = self.conv_in(x)\n",
    "        saved = [x]\n",
    "        for block in self.downs: x = block(x, emb)\n",
    "        saved += [p for o in self.downs for p in o.saved]\n",
    "        x = self.mid_block(x, emb)\n",
    "        for block in self.ups: x = block(x, emb, saved)\n",
    "        return self.conv_out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fc6f4a-2ac4-4851-a401-5740a647e59e",
   "metadata": {},
   "source": [
    "### Export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a83ec-d687-4b0d-876c-c53d83f425db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13116cf-447c-48bc-b898-409b08630898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
