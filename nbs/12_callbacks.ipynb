{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f3ed2c-5f17-46b7-ac9a-b05e0d2459ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf218f-fe7a-4cd8-bcfa-074ce81e41fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import fastcore.all as fc\n",
    "from torch.optim import lr_scheduler\n",
    "from torcheval.metrics import MulticlassAccuracy,Mean\n",
    "from copy import copy\n",
    "from fastprogress import progress_bar,master_bar\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from miniai.utils import def_device, to_device, to_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29a509e-aaf9-439b-84ff-1ac9223e043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CancelFitException(Exception): pass\n",
    "class CancelBatchException(Exception): pass\n",
    "class CancelEpochException(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511935e9-085b-4fb5-baad-3da4160d67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Callback(): order = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9b9be0-1b59-4845-9252-1ccfee7b19df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SingleBatchCB(Callback):\n",
    "    order = 1\n",
    "    def after_batch(self, learn): raise CancelFitException()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544b69ed-10fd-41e7-9907-80d62e463379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MetricsCB(Callback):\n",
    "    def __init__(self, *ms, **metrics):\n",
    "        for o in ms: metrics[type(o).__name__] = o\n",
    "        self.metrics = metrics\n",
    "        self.all_metrics = copy(metrics)\n",
    "        self.all_metrics['loss'] = self.loss = Mean()\n",
    "\n",
    "    def _log(self, d): print(d)\n",
    "    def before_fit(self, learn): learn.metrics = self\n",
    "    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]\n",
    "\n",
    "    def after_epoch(self, learn):\n",
    "        log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}\n",
    "        log['epoch'] = str(learn.epoch)\n",
    "        log['train'] = 'train' if learn.model.training else 'eval'\n",
    "        self._log(log)\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        x,y,*_ = to_cpu(learn.batch)\n",
    "        for m in self.metrics.values(): m.update(to_cpu(learn.preds), y)\n",
    "        self.loss.update(to_cpu(learn.loss), weight=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aad972-dd64-4ef4-9188-81fd7c142be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DeviceCB(Callback):\n",
    "    def __init__(self, device=def_device): fc.store_attr()\n",
    "    def before_fit(self, learn):\n",
    "        if hasattr(learn.model, 'to'): learn.model.to(self.device)\n",
    "    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b00f71-66c5-44de-9b76-aba376fb2162",
   "metadata": {},
   "source": [
    "TrainCB is the default implementation for predict, get_loss, backward, step and zero_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556ffb4a-8939-4657-9276-1b355b856301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TrainCB(Callback):\n",
    "    def __init__(self, n_inp=1): self.n_inp = n_inp\n",
    "    def predict(self, learn): learn.preds = learn.model(*learn.batch[:self.n_inp])\n",
    "    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds, *learn.batch[self.n_inp:])\n",
    "    def backward(self, learn): learn.loss.backward()\n",
    "    def step(self, learn): learn.opt.step()\n",
    "    def zero_grad(self, learn): learn.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc9b782-49bc-471f-8fe8-a118278cb094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ProgressCB(Callback):\n",
    "    order = MetricsCB.order+1\n",
    "    def __init__(self, plot=False): self.plot = plot\n",
    "    def before_fit(self, learn):\n",
    "        learn.epochs = self.mbar = master_bar(learn.epochs)\n",
    "        self.first = True\n",
    "        if hasattr(learn, 'metrics'): learn.metrics._log = self._log\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def _log(self, d):\n",
    "        if self.first:\n",
    "            self.mbar.write(list(d), table=True)\n",
    "            self.first = False\n",
    "        self.mbar.write(list(d.values()), table=True)\n",
    "\n",
    "    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n",
    "    def after_batch(self, learn):\n",
    "        learn.dl.comment = f'{learn.loss:.3f}'\n",
    "        if self.plot and hasattr(learn, 'metrics') and learn.training:\n",
    "            self.losses.append(learn.loss.item())\n",
    "            if self.val_losses: self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])\n",
    "    \n",
    "    def after_epoch(self, learn): \n",
    "        if not learn.training:\n",
    "            if self.plot and hasattr(learn, 'metrics'): \n",
    "                self.val_losses.append(learn.metrics.all_metrics['loss'].compute())\n",
    "                self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch+1).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54354f3b-c09b-413d-ba46-9069ac31a1b6",
   "metadata": {},
   "source": [
    "### Scheduling callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9644bb8e-28fb-4ac4-97ff-6653663c2ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class BaseSchedCB(Callback):\n",
    "    \"\"\" Base scheduling class that will trigger events.  Initialised with a Pytorch type scheduler\n",
    "    The scheduler is assigned to the learner optimiser before fit and the scheduler step method\n",
    "    implemented whenever the optimiser is stepped by the learner\n",
    "    \"\"\"\n",
    "    def __init__(self, sched, **kwargs): \n",
    "        self.sched = sched\n",
    "        self.kwargs=kwargs\n",
    "        \n",
    "    def before_fit(self, learn): self.schedo = self.sched(learn.opt)\n",
    "    def _step(self, learn):\n",
    "        if learn.training: self.schedo.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d38c9d7-a0ef-4ff6-b56a-dd9c77e22540",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class BatchSchedCB(BaseSchedCB):\n",
    "    \"\"\" Inherits from the BaseSchedCD class and adds an additional scheduler step after each batch\n",
    "    \"\"\"\n",
    "    def after_batch(self, learn): self._step(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e1968a-3a0e-4bd3-a25d-fbcf71013e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class OneCycleLrCB(BatchSchedCB):\n",
    "    \"\"\" Inherits from the BatchSchedCD class and works out the scheduler to cover the period of \n",
    "    the fit (ie the number of epoch and batches in a specific fit\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_lr: float = 0.01, **kwargs):\n",
    "        \"\"\" Define the parameters for the one cyc\n",
    "        \"\"\"\n",
    "        self.max_lr = max_lr\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def before_fit(self, learn):\n",
    "        # Assign scheduler\n",
    "        num_batches = len(learn.dls.train)\n",
    "        total_steps = num_batches * learn.n_epochs\n",
    "        self.schedo = lr_scheduler.OneCycleLR(learn.opt, total_steps=total_steps,\n",
    "                         max_lr=self.max_lr, **self.kwargs)\n",
    "        \n",
    "    def after_batch(self, learn): self._step(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3294c929-895d-4c2a-9464-0d1ce57b2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class EpochSchedCB(BaseSchedCB):\n",
    "    \"\"\" Inherits from the BaseSchedCD class and adds an additional scheduler step after each epoch.\n",
    "    Provides a more granular change in the optimiser parameters than the batch scheduler for where\n",
    "    that would be valuable\n",
    "    \"\"\"\n",
    "    def after_epoch(self, learn): self._step(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b954b6-c60c-4ce6-9cae-7726e5a7c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class HasLearnCB(Callback):\n",
    "    def before_fit(self, learn): self.learn = learn \n",
    "    def after_fit(self, learn): self.learn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bef66f-c719-4c79-b684-c503f796891a",
   "metadata": {},
   "source": [
    "### Recording / Logging Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a7b7b-808c-420a-9a90-1412728524c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RecorderCB(Callback):\n",
    "    \"\"\" Class to record specific items during training.  Examples\n",
    "    of possible cases would be:\n",
    "    \n",
    "    RecorderCB(lr=lr_) where lr_ is a callable that will return from the \n",
    "    learner the value to be saved as lr\n",
    "    \n",
    "    In the above case lr_ could be as follows:\n",
    "    \n",
    "        def lr_(cb): cb.pg['lr']\n",
    "        \n",
    "    would then return the parameter group value for 'lr'.  The same could be done for \n",
    "    momentum...\n",
    "    \n",
    "    This reliess upon the way in which the learner is setup as self for callbacks\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, **d): \n",
    "        \"\"\" Define parameters to track\n",
    "        \"\"\"\n",
    "        self.d = d\n",
    "        \n",
    "    def before_fit(self, learn):\n",
    "        # Create empty lists for each keyword\n",
    "        self.recs = {k:[] for k in self.d}\n",
    "        # Define the parameter group as the first one (it there are multiple the others\n",
    "        # will be ignored at present)\n",
    "        self.pg = learn.opt.param_groups[0]\n",
    "    \n",
    "    def after_batch(self, learn):\n",
    "        \"\"\" Append values to be recorded afer each batch using the callable assiciated \n",
    "        with each keyword\n",
    "        \"\"\"\n",
    "        if not learn.training: return\n",
    "        for k,v in self.d.items():\n",
    "            self.recs[k].append(v(self))\n",
    "\n",
    "    def plot(self):\n",
    "        for k,v in self.recs.items():\n",
    "            plt.plot(v, label=k)\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3512568c-a0e5-4c1c-aeb5-e63a6f07e191",
   "metadata": {},
   "source": [
    "### Export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900fe9f-457e-4152-8fae-3a7bd6eebe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988f2202-4b92-48ac-8b49-8cacf35fa79e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniai_env",
   "language": "python",
   "name": "miniai_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
